<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<blockquote>
<p>Layer Normalizaiton is just the &quot;transpose of batch normalizaion&quot;.</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>transpose the input, and set the factor of moving average of mean and std to 0. Then the calculations in this layer is the same as batch norm.</li>
<li>layer normalization dose not need to save the mean and std, and the training and testing process are the same.</li>
<li>It seems that many codes can be resued between batch normalization and layer normalizaiton.</li>
</ol>
<h2 id="forward">forward</h2>
<ul>
<li><span class="math inline">\(x_i\)</span> is the vector representation of the summed inputs to the neurons in a layer <span class="math display">\[\mathbf{x} = \mathbf{W}^T\mathbf{h}\]</span></li>
<li><span class="math inline">\(H\)</span> is number of neurons in the layer</li>
<li><span class="math inline">\(m\)</span> is number of training samples in one mini-batch</li>
<li><p>in mini-batch training, <span class="math inline">\(\mathbf{x}\)</span> is a matrix whose size is: <span class="math inline">\(m \times H\)</span></p></li>
<li><p>In the forward pass, first compute the layer normalization statistics over the hidden units in the same layer: <span class="math display">\[\begin{split}
&amp;\mu &amp;= \frac{1}{H}\sum_{i=1}^{H} x_i \\
&amp;\sigma^2 &amp;= \frac{1}{H}\sum_{i=1}^H(x_i - \mu)^2
\end{split}\]</span></p></li>
<li><p>normalize the output: <span class="math display">\[\begin{split}
&amp;\mathbf{\hat x} &amp;= \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
&amp;\mathbf{y} &amp;= \gamma \odot \mathbf{\hat{x}} + \beta
\end{split}\]</span></p></li>
</ul>
<p><strong>Note that both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are matrix and they have the same size as <span class="math inline">\(\mathbf{\hat{x}}_{m \times H}\)</span></strong></p>
<h3 id="some-notes">Some Notes:</h3>
<ul>
<li>learnable parameters: <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span></li>
<li>all the hidden units in a layer share the same normalization terms</li>
<li>different training sample have different normalization terms</li>
</ul>
<h2 id="backward">backward</h2>
<ul>
<li>the backward pass computes two things:
<ol style="list-style-type: decimal">
<li>partial derivatives of loss function with regard to learnable parameters: <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \gamma}\)</span> and <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \beta}\)</span></li>
<li>partial derivatives of the loss function with regard to input: <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{x}}\)</span></li>
</ol></li>
</ul>
<h3 id="partial-derivatives-of-mathcalmathcall-with-respect-to-learnable-parameters">1. partial derivatives of <span class="math inline">\(\mathcal{\mathcal{L}}\)</span> with respect to learnable parameters:</h3>
<p><span class="math display">\[\begin{eqnarray*}
&amp;\frac{\partial \mathcal{L}}{\partial \mathbf{\gamma}} &amp;= \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial y_i} \odot \mathbf{\hat{x_i}} \tag{1}\\
&amp;\frac{\partial \mathcal{L}}{\partial \mathbf{\beta}} &amp;= \sum_{i=1}^{m} \frac{\partial \mathcal{L}}{\partial y_i} \tag{2}
\end{eqnarray*}\]</span></p>
<h3 id="partial-derivative-of-mathcall-with-respect-to-input-mathbfx">2. partial derivative of <span class="math inline">\(\mathcal{L}\)</span> with respect to input <span class="math inline">\(\mathbf{x}\)</span>:</h3>
<ul>
<li><span class="math inline">\(\mathcal{L}\)</span> can be regarded as a function of <span class="math inline">\(\mathcal{L}(\mathbf{\hat{x}}, \sigma^2, \mu)\)</span></li>
<li><span class="math inline">\(\mathbf{\hat{x}}\)</span>, <span class="math inline">\(\sigma^2\)</span>, and <span class="math inline">\(\mu\)</span> are all functions of <span class="math inline">\(\mathbf{x}\)</span></li>
<li>according to the chain rule, we can obtain the following formular:</li>
</ul>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{x}} = \frac{\partial \mathcal{L}}{\partial \mathbf{\hat{x}}}\frac{\partial \mathbf{\hat x}}{\partial \mathbf{x}} + \frac{\partial \mathcal{L}}{\partial \mu}\frac{\partial \mu }{\partial \mathbf{x}} + \frac{\partial \mathcal{L}}{\partial \sigma ^2}\frac{\partial \sigma^2}{\partial \mathbf{x}}
\tag{3}
\]</span></p>
<p><strong>let's calculate <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{x}}\)</span> step by step.</strong></p>
<h4 id="the-first-part">1. the first part:</h4>
<p><span class="math display">\[\begin{split}
&amp;\frac{\partial \mathcal{L}}{\partial \mathbf{\hat{x}}} &amp;= \frac{\partial \mathcal{L}}{\partial y} \odot \gamma \\
&amp;\frac{\partial \mathbf{\hat x}}{\partial \mathbf{x}} &amp;= (\sigma^2 + \epsilon)^{-\frac{1}{2}}
\end{split}\]</span></p>
<h4 id="the-second-part">2. the second part:</h4>
<p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial \mu} = \frac{\partial \mathcal{L}}{\partial \mathbf{\hat{x}}} \frac{\partial{\mathbf{\hat{x}}}}{\partial \mu} + \frac{\partial \mathcal{L}}{\partial \sigma^2}\frac{\partial \sigma^2}{\partial \mu} \tag{4}\]</span></p>
<p><span class="math display">\[\frac{\partial \mathbf{\hat{x}}}{\partial \mu} =  -(\sigma^2 + \epsilon)^{-\frac{1}{2}}
\tag{5}\]</span></p>
<p><span class="math display">\[\begin{split}
\frac{\partial \sigma^2}{\partial \mu} &amp;= \frac{-2}{H} \sum_{i=1}^{H}(x_i - \mu) \\
&amp;=\frac{-2}{H}(\sum_{i=1}^{H}x_i - \sum_{i=1}^{H} \mu) \\
&amp;=0
\end{split} \tag{6}\]</span></p>
<ul>
<li><p>substitude <span class="math inline">\((5)\)</span>, <span class="math inline">\((6)\)</span> into <span class="math inline">\((4)\)</span>: <span class="math display">\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mu_i} &amp;&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{\hat{x}}} \frac{\partial{\mathbf{\hat{x}}}}{\partial \mu_i } \\
&amp;&amp;=  -(\sigma_i^2 + \epsilon)^{-\frac{1}{2}} \sum_{j=1}^{H}\frac{\partial \mathcal{L}}{\partial {\mathbf{\hat{x_j}}}}
\end{split} \tag{7}\]</span></p></li>
<li><p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu}\)</span> is a matrix shown as below, and equation (<span class="math inline">\(7\)</span>) is one row of this matrix. <span class="math display">\[\left(
\begin{array}{ccc}
\frac{\partial \mathcal{L}}{\partial \mu_1} &amp; ... &amp; \frac{\partial \mathcal{L}}{\partial \mu_1} \\
 &amp; ... &amp;  \\
\frac{\partial \mathcal{L}}{\partial \mu_m} &amp; ... &amp; \frac{\partial \mathcal{L}}{\partial \mu_m} \\
\end{array}
\right)\]</span></p></li>
<li><p>it is easy to get: <span class="math display">\[\frac{\partial \mu}{\partial \mathbf{x}} = \mathbf{\frac{1}{H}}
\tag{8}\]</span></p></li>
</ul>
<h4 id="the-third-part">3. the third part</h4>
<p><span class="math display">\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \sigma_i^2} &amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{\hat{x}}}\frac{\partial \mathbf{\hat{x}}}{\partial \sigma_i^2} \\
 &amp;=-\frac{1}{2}(\sigma_i^2+\epsilon)^{\frac{3}{2}} \sum_{j=1}^{H}&amp;\frac{\partial \mathcal L}{\partial x_j}(x_j - \mu)
\end{split} \tag{9}\]</span></p>
<ul>
<li><p><span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \sigma^2}\)</span> is a matrix as shown below, and equation (<span class="math inline">\(9\)</span>) is a row of this matrix. <span class="math display">\[\left(
\begin{array}{ccc}
\frac{\partial \mathcal{L}}{\partial \sigma_1^2} &amp; ... &amp; \frac{\partial \mathcal{L}}{\partial \sigma_1^2} \\
 &amp; ... &amp;  \\
\frac{\partial \mathcal{L}}{\partial \sigma_m^2} &amp; ... &amp; \frac{\partial \mathcal{L}}{\partial \sigma_m^2} \\
\end{array}
\right)\]</span></p></li>
<li><p>for <span class="math inline">\(\frac{\partial \sigma^2}{\partial x}\)</span>: <span class="math display">\[\frac{\partial \sigma^2}{\partial \mathbf{x}} = \frac{2}{H}(x_j - \mu),
j = i ... H \tag{10}\]</span></p></li>
<li><p><span class="math inline">\(\frac{\partial \sigma^2}{\partial x}\)</span> is a matrix shown as below, and equation (<span class="math inline">\(10\)</span>) is one row of this matrix: <span class="math display">\[\left(
\begin{array}{ccc}
\frac{\partial \sigma_1^2}{\partial x_{1,1}} &amp; ... &amp; \frac{\partial \sigma_1^2}{\partial x_{1,H}} \\
 &amp; ... &amp;  \\
\frac{\partial \sigma_H^2}{\partial x_{m,1}} &amp; ... &amp; \frac{\partial \sigma_H^2}{\partial x_{m,H}} \\
\end{array}
\right)\]</span></p></li>
</ul>
</body>
</html>
